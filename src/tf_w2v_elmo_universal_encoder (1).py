# -*- coding: utf-8 -*-
"""Copy of HS_Dataset_on_small_Cross_Valid_TF_W2V_ELMo_Universal_Encoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DdQxZwSux2LmuP3utMi4W0NVDSWbVkHw

## Import Libraries
"""

# Install packages
# !pip install xlrd
# !pip install gensim
# !pip install -U nltk
# !pip install spacy

"""## Access to GDrive"""

# Access to resources
from google.colab import drive
# drive.mount('/content/gdrive', force_remount=True)

"""# Data Cleaning and Preprocessing"""

# Load stop words and more (will be added later)
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print(stop_words)
# Read https://machinelearningmastery.com/clean-text-machine-learning-python/
# for cleaning text

"""## Cleaning dataset functions"""

# Cleaning data
import unicodedata
import re
import gensim
from nltk import tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

del stop_words[stop_words.index('not')]
del stop_words[stop_words.index('your')]

# Remove unwanted noise
stop_words.append('rt')
stop_words.append('wow')
stop_words.append('ok')
stop_words.append('mo')
stop_words.append('dm')
stop_words.append('idgaf')

CONTRACTION_MAP = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he had",
"he'd've": "he would have",
"he'll": "hehe will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "hey would",
"they'd've": "they would have",
"they'll": "tthey will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what'll've": "what will have",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"when's": "when is",
"when've": "when have",
"where'd": "where did",
"where's": "where is",
"where've": "where have",
"who'll": "who will",
"who'll've": "who will have",
"who's": "who is",
"who've": "who have",
"why's": "why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you would",
"you'd've": "you would have",
"you'll": "you will",
"you'll've": "you will have",
"you're": "you are",
"you've": "you have",
"fvcking": "fucking",
"fking": "fucking",
# Correction
    # "ill": "I will",
    "seriois": "serious",
    #"mo": "my",
    #"ass": "asss",
    "lmaoooooooo": "lmao",
    "uncomf": "uncomfortable",
    "pls": "please",
    "lowlife": "low life",
    "puss": "pussy",
    # Hashtash expand
    "ihatefemales": "I hate females",
    "yesallwomen": "yes all women",
    "sendthemhome": "send them home",
    "stoptheinvasion": "stop the invation",
    "buildthewall": "buid the wall",
    "womensuck": "women suck",
    "stopimmigration": "stop immigration",
    "sendthemback": "send them back"
}

# Expand the contractions
def expand_contractions(s, contractions_dict=CONTRACTION_MAP):
    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, s)

# Sentences to words
def sent_to_words(sentences):
    for sentence in sentences:
        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))
        
# tokenize
def sent_tokenize(sents):
    return tokenize.sent_tokenize(sents)
  
# Function to remove some unwanted info
def remove_info(data, remove_list):
    for i, sent in enumerate(data):
        for com in remove_list:
            if com.strip() != '':
                sent = sent.lower().replace(com.lower(), '')
        data[i] = sent
    return data

# Remove special characters; only get ASCII
def remove_accents(input_str):
    nfkd_form = unicodedata.normalize('NFKD', str(input_str))
    only_ascii = nfkd_form.encode('ASCII', 'ignore')
    only_ascii = str(only_ascii)[2:-1]
    return only_ascii

# Remove emails, link, tweeter account, ..
def clean(texts):
    # Remove Emails
    data = [re.sub('\S*@\S*\s?', '', sent.lower().strip()) for sent in texts]
    # Remove @username
    data = [re.sub('@\S*\s?', '', sent) for sent in data]
    # Remove link
    data = [re.sub(r'http\S+', '', sent) for sent in data]
    # Remove new line characters
    data = [re.sub('\s+', ' ', sent) for sent in data]
    # Remove multi dots
    data = [re.sub('\.\.', '', sent) for sent in data]
    # Remove distracting quotes
    # data = [re.sub("\"", "", sent) for sent in data]
    # Remove #
    data = [re.sub("#", "", sent) for sent in data]
    # Remove number
    data = [re.sub(r"\d+", '', sent) for sent in data]
    return data

# Func to remove stop words
def remove_stopwords(texts, stop_words=stop_words):
    return [' '.join([word.replace(' ', '') 
                for word in gensim.utils.simple_preprocess(str(doc)) \
                if word.replace(' ', '') not in stop_words]) for doc in texts]

# Func to lemmatize words using spacy
def lemmatization(texts, stop_words=None, 
                  allowed_postags=('NOUN', 'VERB', 'ADV')):
    nlp = en_core_web_md.load()
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc 
                            if token.pos_ in allowed_postags \
                              and token.lemma_ not in stop_words])
    return texts_out

# Func to lemmatize words using nltk
# Refer to https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
def lemmatization_nltk(sents):
    # input: sents -> list of sentences
    # output: list of lemmatized words
    def get_wordnet_pos(word):
        """Map POS tag to first character lemmatize() accepts"""
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV}

        return tag_dict.get(tag, wordnet.NOUN)

    lemmatized_words = []
    
    lemmatizer = WordNetLemmatizer()
    for sentence in sents:
        lemmatized_words.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) 
                                  for w in nltk.word_tokenize(sentence)]))
    return lemmatized_words

# Normalized pipeline
def normalization_pipeline(texts):
    normalized_texts = []
    # lower text
    normalized_texts = [text.lower() for text in texts]
    
    # Clean text: special words, numbers, ..
    normalized_texts = clean(normalized_texts)
    # Expand contraction 
    normalized_texts = [expand_contractions(text) for text in normalized_texts]
    normailzed_texts = [remove_accents(text) for text in normalized_texts]
    # Break paragraph into sentents
    normalized_texts = [sent_tokenize(text) for text in normalized_texts]
    
    # Remove stopwords
    normalized_texts = remove_stopwords(normalized_texts)
    
    normalized_texts = lemmatization_nltk(normalized_texts)
    return normalized_texts

# Read data from file
import pandas as pd

# Point to the file in Google Drive
# filename='/content/gdrive/My Drive/trial_en.tsv'
filename = '/home/zahorack/Projects/code-against-hate/big_dataset.csv'

df = pd.read_csv(filename, sep='\t')
corpus = df['text'].values

normalized_texts = normalization_pipeline(corpus)

df['normalized_text'] = normalized_texts
print(df.head())

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(min_df=2,
                           norm='l2',
                           smooth_idf=True,
                           use_idf=True,
                           ngram_range=(1, 1))
features = vectorizer.fit_transform(corpus)
print(features.todense())

"""# Feature enginerring
Bow, TF-IDF and Word2Vec
"""

# Feature extraction
# Use TF and TF-IDF from scikit-learn

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import pandas as pd

def bow_extractor(corpus, ngram_range=(2,2)):
    vectorizer = CountVectorizer(min_df=2, ngram_range=ngram_range)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features

def tfidf_extractor(corpus, ngram_range=(2,2)):
    vectorizer = TfidfVectorizer(min_df=2,
                               norm='l2',
                               smooth_idf=True,
                               use_idf=True,
                               ngram_range=ngram_range)
    features = vectorizer.fit_transform(corpus)
    return vectorizer, features
  
def tfidf_transformer(bow_matrix):
    transformer = TfidfTransformer(norm='l2',
                                 smooth_idf=True,
                                 use_idf=True)
    tfidf_matrix = transformer.fit_transform(bow_matrix)
    return transformer, tfidf_matrix

def display_features(features, feature_names):
    df = pd.DataFrame(data=features,
                    columns=feature_names)
    print(df)

# Demo Feature extraction with count vector and tf-idf
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

vectorizer = CountVectorizer()
train_data_features = vectorizer.fit_transform(df['normalized_text'].values.copy())
vocab = vectorizer.get_feature_names()

# Sum up the counts of each vocabulary word
dist = np.sum(train_data_features.toarray(), axis=0)

# Print Word Count
# for tag, count in zip(vocab, dist):
#    print(count, tag)

corpus = df['normalized_text'].values.copy()
bow_vect, bow_features = bow_extractor(corpus)
features = bow_features.todense()
feature_names = bow_vect.get_feature_names()
# display_features(features, feature_names)

# TF-IDF
tfidf_vectorizer, tdidf_features = tfidf_extractor(corpus.copy())
features = tdidf_features.todense()
feature_names = tfidf_vectorizer.get_feature_names()

#display_features(np.round(tdidf_features.todense(), 2), feature_names)

# word2vec feature extraction
from gensim.models import Word2Vec
import nltk
import numpy as np
np.random.seed(1)

# define function to average word vectors for a text document    
def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,),dtype="float32")
    nwords = 0.
    for word in words:
        if word in vocabulary:
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model[word])
    if nwords:
        feature_vector = np.divide(feature_vector, nwords)
    return feature_vector

# generalize above function for a corpus of documents  
def averaged_word_vectorizer(corpus, model, num_features):
    vocabulary = set(model.wv.index2word)
    features = [average_word_vectors(tokenized_sentence, model, 
                                   vocabulary, num_features)
                  for tokenized_sentence in corpus]
    return np.array(features)

# define function to compute tfidf weighted averaged word vector for a document
def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):
    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] 
                 if tfidf_vocabulary.get(word) else 0 for word in words]
    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}
    feature_vector = np.zeros((num_features,),dtype="float32")
    vocabulary = set(model.wv.index2word)
    wts = 0.
    for word in words:
        if word in vocabulary:
            word_vector = model[word]
            weighted_word_vector = word_tfidf_map[word] * word_vector
            wts = wts + word_tfidf_map[word]
        feature_vector = np.add(feature_vector, weighted_word_vector)
    if wts:
        feature_vector = np.divide(feature_vector, wts)
    return feature_vector

# generalize above function for a corpus of documents    
def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, tfidf_vocabulary, model, num_features):
    docs_tfidfs = [(doc, doc_tfidf)
                 for doc, doc_tfidf
                 in zip(corpus, tfidf_vectors)]
    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,
                                 model, num_features)
                  for tokenized_sentence, tfidf in docs_tfidfs]
    return np.array(features)


corpus = df['normalized_text'].values.copy()
tokenized_corpus = [nltk.word_tokenize(sent) for sent in corpus]


model = Word2Vec(tokenized_corpus, min_count=2, workers=4)
avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_corpus,
                                                 model=model,
                                                 num_features=100)

# print(avg_word_vec_features)

print(np.round(avg_word_vec_features, 5))

"""## Preparing Data before using models"""

# Prepare data for modeling

from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
np.random.seed(1)

def prepare_datasets(corpus, labels, test_data_proportion=0.3):
    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,
                                                      test_size=test_data_proportion, 
                                                      random_state=42,
                                                      shuffle=True)
    return train_X, test_X, train_Y, test_Y

# Defining prediction measure scores

def get_metrics(true_labels, predicted_labels):
    print('Accuracy:', 
        np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))
    print('Precision:', 
        np.round(metrics.precision_score(true_labels,
                                         predicted_labels,
                                         average='weighted'),2))
    print('Recall:', 
        np.round(metrics.recall_score(true_labels,
                                      predicted_labels,
                                      average='weighted'),2))
    print('F1 Score:', 
        np.round(metrics.f1_score(true_labels,
                                  predicted_labels,
                                  average='weighted'),2))
    return

def train_predict_evaluate_model(classifier, train_features, 
                                 train_labels, test_features, test_labels):
    # build model    
    classifier.fit(train_features, train_labels)
    # predict using model
    predictions = classifier.predict(test_features)
    # evaluate model prediction performance  
#     get_metrics(true_labels=test_labels,
#               predicted_labels=predictions)

    return predictions

"""## Defined Accuracy Report"""

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

def accuracy_report(yvalid, prediction):
    cm1 = confusion_matrix(yvalid, prediction)
    print('Confusion Matrix :', cm1)

    total1=sum(sum(cm1))

    accuracy1=(cm1[0,0]+cm1[1,1])/total1
    print ('Accuracy : ', accuracy1)

    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    print('Sensitivity : ', sensitivity1 )

    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    print('Specificity : ', specificity1)
    print('                 ')
    print('Accuracy:', accuracy_score(yvalid, prediction))
    print('Precision:', precision_score(yvalid, prediction))
    print('Recall:', recall_score(yvalid, prediction))
    print('F1:', f1_score(yvalid, prediction))
    return accuracy_score(yvalid, prediction), f1_score(yvalid, prediction), \
            recall_score(yvalid, prediction), precision_score(yvalid, prediction)

"""# Cross-validation with TFIDF and Word2Vec

Support vector machine, Logistic Regression with TFIDF and Word2Vec

## SVM

### SVM Cross validation with Word2Vec
"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate

corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels
# averaged word vector features
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []



for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = gensim.models.Word2Vec(X,
                                   size=500, 
                                   min_count=2,
                                   window=5, 
                                   workers=4)
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    svm = SVC()
    # Averaged word vector
    print('SVM results with AVG word vector')
    # Support Vector Machine with averaged word vector features use word2vec
    svm_predictions = train_predict_evaluate_model(classifier=svm, 
                                                 train_features=avg_wv_train_features, 
                                                 train_labels=y_train,
                                                 test_features=avg_wv_test_features, 
                                                 test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, svm_predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('SVM Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### SVM cross validation with TFIDF"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate

corpus = df['normalized_text'].values
labels = df['HS'].values

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=4, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, 
                                   max_features=200, 
                                   stop_words='english')
    
    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)
    
    svm = SVC()
    predictions = train_predict_evaluate_model(classifier=svm,
                                               train_features=tfidf_train_features, 
                                               train_labels=y_train,
                                               test_features=tfidf_test_features, 
                                               test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('SVM Overall with TFIDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## Logistic Regression

### Losgistic Regression cross validation with Word2Vec
"""

# Defining model
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate

corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

model = gensim.models.Word2Vec(X,
                               size=500, 
                               min_count=2,
                               window=5, 
                               workers=4)

for train_index, test_index in kf.split(X):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    logistic = LogisticRegression()
    # Averaged word vector
    print('Logistic Regression results with AVG word vector')
    # Support Vector Machine with averaged word vector features use word2vec
    predictions = train_predict_evaluate_model(classifier=logistic, 
                                               train_features=avg_wv_train_features, 
                                               train_labels=y_train,
                                               test_features=avg_wv_test_features, 
                                               test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('Logistic Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### Logistic cross validation with TFIDF"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate

corpus = df['normalized_text'].values
labels = df['HS'].values

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=22, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, 
                                   max_features=500, 
                                   stop_words='english')
    
    tfidf_train_features = tfidf_vectorizer.fit_transform(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)
    
    logistic = LogisticRegression()
    predictions = train_predict_evaluate_model(classifier=logistic, 
                                               train_features=tfidf_train_features, 
                                               train_labels=y_train,
                                               test_features=tfidf_test_features, 
                                               test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('Ligistic Regression Overall with TFIDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## Feedforward NN Cross validation
This section shows the results for FF NN with Word2Vec and TFIDF
"""

# TF-IDF with FFNN - defining the model

import math
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Activation
from keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error
np.random.seed(1)


class MLP(object):
    def __init__(self,n_hidden, input_shape, n_epochs=120):
        """TODO: Contructor for MLP
            @args: the Train data and layers info to build NN """

        # Build MLP
        self.n_epochs=n_epochs
        
        model = Sequential()
        model.add(Dense(n_hidden[0], input_shape=input_shape))
        model.add(Activation('relu'))
        for i in range(len(n_hidden)):
            model.add(Dense(n_hidden[i]))
            model.add(Activation('relu'))
        model.add(Dense(1))
        model.add(Activation('sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='adam', 
                      metrics=['accuracy'])
        self.early_stop = EarlyStopping(monitor='val_loss', patience=20,
                                   mode='auto')
        
        self.model = model
        
    def fit(self, X_train, y_train):
        self.model.fit(X_train, y_train, verbose=0,
                  batch_size=10,
                  epochs=self.n_epochs,
                  shuffle=True
                  )
        return self
      
    def predict(self, X_test):
        y_predict = self.model.predict(X_test)
        y_predict = y_predict.reshape(-1, 1).ravel()
        return y_predict

    def evaluate(self, X_val, y_val):
        y_val = y_val.reshape(-1, 1)
        score = self.model.evaluate(X_val, y_val)
        return score
      
    def load(self, name):
        self.model.load_weights(name)
        print('Load model weights successfully, ', name)
        return self
                          

    def save(self, name):
        self.model.save_weights(name)
        print('Save model weights successfully, ', name)
        return self

"""### TFIDF"""

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)

    xtrain_tfidf, ytrain = tfidf_train_features, y_train
    xvalid_tfidf, yvalid = tfidf_test_features, y_test
    mlp = MLP(n_hidden=[256, 256], input_shape=(xtrain_tfidf.shape[1],))
    mlp.fit(xtrain_tfidf, ytrain)
    mlp_predict = mlp.predict(xvalid_tfidf)

    mlp_predict[mlp_predict >= 0.5] = 1
    mlp_predict[mlp_predict < 0.5] = 0

    acc, f1, recall, precision = accuracy_report(y_test, mlp_predict)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('FFNN Overall with TFIDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### Word2Vec"""

corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = gensim.models.Word2Vec(x_train,
                                   size=500, 
                                   min_count=2,
                                   window=5, 
                                   workers=4)
    
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    
    xtrain_wv, ytrain = avg_wv_train_features, y_train
    xvalid_wv, yvalid = avg_wv_test_features, y_test
    mlp = MLP(n_hidden=[256, 256], input_shape=(xtrain_wv.shape[1],))
    mlp.fit(xtrain_wv, ytrain)
    mlp_predict = mlp.predict(xvalid_wv)

    mlp_predict[mlp_predict >= 0.5] = 1
    mlp_predict[mlp_predict < 0.5] = 0

    acc, f1, recall, precision = accuracy_report(y_test, mlp_predict)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('FFNN Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## MC Dropout cross validation"""

import warnings
import numpy as np
from keras import backend as K
from keras.models import Sequential
from keras.layers.core import Dense
from keras.layers.core import Dropout
from keras.regularizers import l2
from keras.callbacks import EarlyStopping

np.random.seed(1234)
warnings.filterwarnings("ignore")

class net:
    def __init__(self, X_train, y_train, n_hidden, n_epochs=200,
                normalize=False):
        X_train = np.reshape(X_train, (X_train.shape[0], 
                                       X_train.shape[1]))
        
        self.mean_y_train = np.mean(y_train)
        self.std_y_train = np.std(y_train)
        
        y_train_normalized = y_train
        N = X_train.shape[0]
        dropout = 0.5
        batch_size = 4
        
        model = Sequential()
        model.add(Dense(n_hidden[0], 
                        input_shape=(X_train.shape[1], ),
                       activation='relu',
                       kernel_regularizer=l2(1e-4),
                       bias_regularizer=l2(1e-4)))
        for i in range(len(n_hidden)):
            model.add(Dense(n_hidden[i], activation='relu',
                           kernel_regularizer=l2(1e-4),
                           bias_regularizer=l2(1e-4)))
            model.add(Dropout(dropout))
        model.add(Dense(1,
                        activation='sigmoid'))
        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics=['acc'])
        model.fit(X_train, y_train_normalized,
                  batch_size=batch_size,
                  nb_epoch=n_epochs,
                  validation_split=0.1,
                  shuffle=True,
                  verbose=0)
        self.model = model
        
    def predict(self, X_test):
#         X_test = (X_test - np.full(X_test.shape, self.mean_X_train)) / \
#                                    np.full(X_test.shape, self.std_X_train)
#         print(X_test.shape)
#         X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))
        model = self.model
        T = 1000
        predict_stochastic = K.function([model.layers[0].input,
                                        K.learning_phase()],
                                        [model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([X_test, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        return MC_pred

"""### TF-IDF"""

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)

    xtrain_tfidf, ytrain = tfidf_train_features, y_train
    xvalid_tfidf, yvalid = tfidf_test_features, y_test
    
    ytrain = np.reshape(ytrain, [-1, 1])
    yvalid = np.reshape(yvalid, [-1, 1])
    mc_net = net(X_train=xtrain_tfidf, y_train=ytrain, 
                 n_hidden=[256, 256], normalize=False,
                 n_epochs=50)

    mc_predict = mc_net.predict(xvalid_tfidf.copy())
    mc_predict = np.reshape(mc_predict, [-1, 1])
    mc_predict[mc_predict >= 0.5] = 1
    mc_predict[mc_predict < 0.5] = 0

    acc, f1, recall, precision = accuracy_report(y_test, mc_predict)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('FFNN Overall with TFIDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### Word2Vec"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = gensim.models.Word2Vec(X,
                               size=500, 
                               min_count=2,
                               window=5, 
                               workers=10)
    
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    
    xtrain_wv, ytrain = avg_wv_train_features, y_train
    xvalid_wv, yvalid = avg_wv_test_features, y_test
    ytrain = np.reshape(ytrain, [-1, 1])
    yvalid = np.reshape(yvalid, [-1, 1])

    mc_net = net(X_train=xtrain_wv, y_train=ytrain, 
                 n_hidden=[512, 512], normalize=False,
                 n_epochs=100)

    mc_predict = mc_net.predict(xvalid_wv)
    mc_predict = np.reshape(mc_predict, [-1, 1])
    mc_predict[mc_predict >= 0.5] = 1
    mc_predict[mc_predict < 0.5] = 0

    acc, f1, recall, precision = accuracy_report(y_test, mc_predict)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)

print('FFNN Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## LSTM with TFIDF and Word2Vec"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D
from keras.layers import LSTM

LSTM_DIM = 1024 # total LSTM units

class LSTM_No_MC():
    def __init__(self):
        self._build_LSTM_model()

    def _build_LSTM_model(self):
        self.model = Sequential() 
        self.model.add(LSTM(LSTM_DIM,
                       recurrent_dropout=0.1)) 
        self.model.add(Dense(1024, activation='relu')) 
        self.model.add(Dense(1, activation='sigmoid'))
        self.model.compile(loss='binary_crossentropy', 
                            optimizer='adam', 
                            metrics=['accuracy'])
        return self
    
    def fit(self, train_X, train_y):
        self.model.fit(train_X, train_y, 
                       epochs=50, 
                       batch_size=4, 
                       shuffle=True, 
                       verbose=0)

    def predict(self, x_test):
        predictions = self.model.predict(x_test)
        return predictions
    
    def evaluate(self, x_test, y_test):
        predictions = self.predict(x_test)
        predictions[predictions >= 0.5] = 1
        predictions[predictions < 0.5] = 0
    
        acc, f1, recall, precision = accuracy_report(y_test, predictions)
        return acc, f1, recall, precision

def lstm_cv(X, y):
    kf = KFold(n_splits=5, random_state=42, 
               shuffle=True)
    count = 0
    mean_acc = []
    f1s = []
    recalls = []
    precis = []
    for train_index, test_index in kf.split(X, y):
        x_train, x_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        input_length = x_train.shape[1]
        x_train = np.reshape(x_train,
                                (x_train.shape[0], 1, 
                                 x_train.shape[1]))

        x_test= np.reshape(x_test, (x_test.shape[0], 1, 
                                    x_test.shape[1]))
        print('*'*50)
        model = LSTM_No_MC()
        model.fit(x_train, y_train)

        # Prediction on the test set.
        print('*'*10)
        print('Evaluation model ', count + 1, '/5')
        count += 1
        # predictions = model.predict(x_test)

        acc, f1, recal, precision = model.evaluate(x_test, y_test)

        mean_acc.append(acc)
        f1s.append(f1)
        recalls.append(recal)
        precis.append(precision)

    print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
    print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
    print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
    print('Precisions=', np.mean(precis), ', std=', np.std(precis))

"""### Word2Vec"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels

kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = gensim.models.Word2Vec(X,
                               size=500, 
                               min_count=2,
                               window=5, 
                               workers=10)
    
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    
    xtrain_wv, ytrain = avg_wv_train_features, y_train
    xvalid_wv, yvalid = avg_wv_test_features, y_test
    
    xtrain_wv = np.reshape(xtrain_wv,
                            (xtrain_wv.shape[0], 1, 
                             xtrain_wv.shape[1]))

    xvalid_wv = np.reshape(xvalid_wv, (xvalid_wv.shape[0], 1, 
                                xvalid_wv.shape[1]))

    model = LSTM_No_MC()
    model.fit(xtrain_wv, ytrain)

    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    # predictions = model.predict(x_test)

    acc, f1, recal, precision = model.evaluate(xvalid_wv, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)

print('LSTM Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### TFIDF"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)

    xtrain_tfidf, ytrain = tfidf_train_features, y_train
    xvalid_tfidf, yvalid = tfidf_test_features, y_test
    
    xtrain_tfidf = np.reshape(xtrain_tfidf,
                            (xtrain_tfidf.shape[0], 1, 
                             xtrain_tfidf.shape[1]))

    xvalid_tfidf = np.reshape(xvalid_tfidf, (xvalid_tfidf.shape[0], 1, 
                                xvalid_tfidf.shape[1]))

    model = LSTM_No_MC()
    model.fit(xtrain_wv, ytrain)

    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1

    acc, f1, recal, precision = model.evaluate(xvalid_wv, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)

print('LSTM MC Dropout Overall with TF-IDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## LSTM Dropout"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D
from keras.layers import LSTM

LSTM_DIM = 1024 # total LSTM units

class LSTM_MC():
    def __init__(self):
        self._build_LSTM_model()

    def _build_LSTM_model(self):
        self.model = Sequential() 
        self.model.add(LSTM(LSTM_DIM,
                            recurrent_dropout=0.1,
                           kernel_regularizer=l2(1e-6),
                           bias_regularizer=l2(1e-6),
                           activity_regularizer=l2(1e-6)))
        self.model.add(Dropout(0.25))
        self.model.add(Dense(1024, activation='relu',
                            kernel_regularizer=l2(1e-6),
                            bias_regularizer=l2(1e-6),
                            activity_regularizer=l2(1e-6)))
        self.model.add(Dropout(0.25))
        self.model.add(Dense(1, activation='sigmoid'))
        self.model.compile(loss='binary_crossentropy', 
                            optimizer='adam', 
                            metrics=['accuracy'])
        return self
    
    def fit(self, train_X, train_y):
        self.model.fit(train_X, train_y, 
              epochs=50, 
              batch_size=4, 
              shuffle=True, 
              verbose=0)

    def predict(self, x_test):
        predictions = self.model.predict(x_test)
        return predictions
    
    def evaluate(self, test_X, y_test):
#         test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        MC_pred = MC_pred.reshape(-1, 1)
        MC_pred[MC_pred >= 0.5] = 1
        MC_pred[MC_pred < 0.5] = 0
    
        acc, f1, recall, precision = accuracy_report(y_test, 
                                                     MC_pred)
        return acc, f1, recall, precision

"""### Word2Vec"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
corpus = df['normalized_text'].values
labels = df['HS'].values

X = [nltk.word_tokenize(text) for text in corpus]
X = np.array(X)
y = labels

kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model = gensim.models.Word2Vec(X,
                               size=500, 
                               min_count=2,
                               window=5, 
                               workers=10)
    
    avg_wv_train_features = averaged_word_vectorizer(corpus=x_train, 
                                                     model=model, 
                                                     num_features=500)
    avg_wv_test_features = averaged_word_vectorizer(corpus=x_test, 
                                                    model=model, 
                                                    num_features=500)
    
    xtrain_wv, ytrain = avg_wv_train_features, y_train
    xvalid_wv, yvalid = avg_wv_test_features, y_test
    
    xtrain_wv = np.reshape(xtrain_wv,
                           (xtrain_wv.shape[0], 1, 
                            xtrain_wv.shape[1]))

    xvalid_wv = np.reshape(xvalid_wv, 
                           (xvalid_wv.shape[0], 1, 
                            xvalid_wv.shape[1]))

    model = LSTM_MC()
    model.fit(xtrain_wv, ytrain)

    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1

    acc, f1, recal, precision = model.evaluate(xvalid_wv, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)

print('LSTM MC Dropout Overall with Word2Vec')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""### TFIDF"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(x_train)
    tfidf_test_features = tfidf_vectorizer.transform(x_test)

    xtrain_tfidf, ytrain = tfidf_train_features, y_train
    xvalid_tfidf, yvalid = tfidf_test_features, y_test
    
    xtrain_tfidf = np.reshape(xtrain_tfidf,
                            (xtrain_tfidf.shape[0], 1, 
                             xtrain_tfidf.shape[1]))

    xvalid_tfidf = np.reshape(xvalid_tfidf, (xvalid_tfidf.shape[0], 1, 
                                xvalid_tfidf.shape[1]))

    model = LSTM_MC()
    model.fit(xtrain_wv, ytrain)

    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1

    acc, f1, recal, precision = model.evaluate(xvalid_wv, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)

print('LSTM MC Dropout Overall with TF-IDF')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""# Dropout Model with Tensorflow"""

# Define the MC Dropout NN model
import numpy as np
import tensorflow as tf
from tensorflow.contrib.distributions import Bernoulli
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
np.random.seed(1)
config = tf.ConfigProto()
config.gpu_options.allow_growth = True

# To define the layer
class VariationalDense:
    """Variational Dense Layer Class"""
    
    def __init__(self, n_in, n_out, model_prob, model_lam):
        self.model_prob = model_prob
        self.model_lam = model_lam
        # sampling the probabilities
        self.model_bern = Bernoulli(probs=self.model_prob, dtype=tf.float32)
        # M is the matrix of the parameters from which we will dropout
        self.model_M = tf.Variable(tf.truncated_normal([n_in, n_out], 
                                                       stddev=0.01))
        # m is the bias
        self.model_m = tf.Variable(tf.zeros([n_out]))
        # We need to sample from the layer which results from the dropout mechanisam 
        self.model_W = tf.matmul(
            tf.diag(self.model_bern.sample((n_in, ))), self.model_M
        )

    # Call function 
    def __call__(self, X, activation=tf.identity):
        output = activation(tf.matmul(X, self.model_W) + self.model_m)
        # If we have regression problem
        if self.model_M.shape[1] == 1:
            output = tf.squeeze(output)
        return output

    # We have special regularization function  
    @property
    def regularization(self):
      # lambda is taking part in the regularization
        return self.model_lam * (
            self.model_prob * tf.reduce_sum(tf.square(self.model_M)) +
            tf.reduce_sum(tf.square(self.model_m))
        )

"""### Train and valid data with TF-IDF"""

# Prepare data for NN
corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,
                                                                       labels)
tfidf_vectorizer, tfidf_train_features = tfidf_extractor(train_corpus.copy())
tfidf_test_features = tfidf_vectorizer.transform(test_corpus.copy())

xtrain_tfidf, ytrain, xvalid_tfidf, yvalid = tfidf_train_features, train_labels, tfidf_test_features, test_labels

ytrain = np.reshape(ytrain, [-1, 1])
yvalid = np.reshape(yvalid, [-1, 1])

N_FEATURES = xtrain_tfidf.shape[1]
# Transform data to put into NN
ytrain = np.reshape(ytrain, [-1, 1]).ravel()
yvalid = np.reshape(yvalid, [-1, 1]).ravel()
xtrain_tfidf = np.reshape(xtrain_tfidf.toarray(), [-1, N_FEATURES])
xvalid_tfidf = np.reshape(xvalid_tfidf.toarray(), [-1, N_FEATURES])

n_samples = xtrain_tfidf.shape[0]
# n_feats is the number of input
n_feats = xtrain_tfidf.shape[1]
# Define number of neurons, dropout val and ..
n_hidden = 128
model_prob = 0.85
model_lam = 1e-3
# Define input and output structure of NN
model_X = tf.placeholder(tf.float32, [None, n_feats])
model_y = tf.placeholder(tf.float32, [None])
# Structure of NN
# 2 hidden layers and 128 neurons for each layer
# Relu activations 
# Sigmoid for output layer
model_L_1 = VariationalDense(n_feats, n_hidden, model_prob, model_lam)
model_L_2 = VariationalDense(n_hidden, n_hidden, model_prob, model_lam)
model_L_2_2 = VariationalDense(n_hidden, n_hidden, model_prob, model_lam)
model_L_3 = VariationalDense(n_hidden, 1, model_prob, model_lam)
model_out_1 = model_L_1(model_X, tf.nn.relu)
model_out_2 = model_L_2(model_out_1, tf.nn.relu)
model_out_2_2 = model_L_2_2(model_out_2, tf.nn.mse)
model_pred = model_L_3(model_out_2_2)
# Define loss function
model_sse = tf.reduce_sum(tf.square(model_y - model_pred))
model_mse = model_sse / n_samples
model_loss = (
    # Negative log-likelihood.
    model_sse +
    # Regularization.
    model_L_1.regularization +
    model_L_2.regularization +
    model_L_2_2.regularization +    
    model_L_3.regularization
) / n_samples
train_step = tf.train.AdamOptimizer(1e-4).minimize(model_loss)

with tf.Session(config=config) as sess:
    # Training part
    sess.run(tf.global_variables_initializer())
    for i in range(10000):
        sess.run(train_step, {model_X: xtrain_tfidf, model_y: ytrain})
        if i % 128 == 0:
            mse = sess.run(model_mse, {model_X: xtrain_tfidf, model_y: ytrain})
    # Sampling
    n_post = 1000
    # Define the post matrix
    Y_post = np.zeros((n_post, xvalid_tfidf.shape[0]))
    for i in range(n_post):
        Y_post[i] = sess.run(model_pred, {model_X: xvalid_tfidf})
y_predict = Y_post.mean(axis=0)
y_predict = np.reshape(y_predict, [-1, 1])
y_predict[y_predict >= 0.5] = 1
y_predict[y_predict < 0.5] = 0
accuracy_report(yvalid, y_predict)

"""## Train and valid with w2v"""

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,
                                                                       labels)

# tokenize documents
tokenized_train = [nltk.word_tokenize(text) for text in train_corpus]
tokenized_test = [nltk.word_tokenize(text) for text in test_corpus]

# build word2vec model         
# Parameter need to tune
model = gensim.models.Word2Vec(tokenized_train, 
                               size=1000, 
                               min_count=5,
                               window=100, 
                               workers=8)
# averaged word vector features
avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train, 
                                                 model=model, 
                                                 num_features=1000)

avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test, 
                                                model=model, 
                                                num_features=1000)


xtrain_features = avg_wv_train_features 
xvalid_features=avg_wv_test_features

yvalid =test_labels
ytrain = train_labels

# n_feats is the number of input
n_samples = xtrain_features.shape[0]
n_feats = xtrain_features.shape[1]

# Transform data to put into NN
ytrain = np.reshape(ytrain, [-1, 1]).ravel()
yvalid = np.reshape(yvalid, [-1, 1]).ravel()
xtrain_features = np.reshape(xtrain_features, [-1, n_feats])
xvalid_features = np.reshape(xvalid_features, [-1, n_feats])

# Define number of neurons, dropout val and ..
n_hidden = 8
model_prob = 0.9
model_lam = 1e-3
# Define input and output structure of NN
model_X = tf.placeholder(tf.float32, [None, n_feats])
model_y = tf.placeholder(tf.float32, [None])
# Structure of NN
# 2 hidden layers and 128 neurons for each layer
# Relu activations 
# Sigmoid for output layer
model_L_1 = VariationalDense(n_feats, n_hidden, model_prob, model_lam)
model_L_2 = VariationalDense(n_hidden, n_hidden, model_prob, model_lam)
model_L_2_2 = VariationalDense(n_hidden, n_hidden, model_prob, model_lam)
model_L_3 = VariationalDense(n_hidden, 1, model_prob, model_lam)
model_out_1 = model_L_1(model_X, tf.nn.relu)
model_out_2 = model_L_2(model_out_1, tf.nn.relu)
model_out_2_2 = model_L_2_2(model_out_2, tf.nn.sigmoid)
model_pred = model_L_3(model_out_2_2)
# Define loss function
model_sse = tf.reduce_sum(tf.square(model_y - model_pred))
model_mse = model_sse / n_samples
model_loss = (
    # Negative log-likelihood.
    model_sse +
    # Regularization.
    model_L_1.regularization +
    model_L_2.regularization +
    model_L_2_2.regularization +    
    model_L_3.regularization
) / n_samples
train_step = tf.train.AdamOptimizer(1e-4).minimize(model_loss)

with tf.Session(config=config) as sess:
    # Training part
    sess.run(tf.global_variables_initializer())
    for i in range(1000):
        sess.run(train_step, {model_X: xtrain_features, model_y: ytrain})
        if i % 128 == 0:
            mse = sess.run(model_mse, {model_X: xtrain_features, model_y: ytrain})
    # Sampling
    n_post = 1000
    # Define the post matrix
    Y_post = np.zeros((n_post, xvalid_features.shape[0]))
    for i in range(n_post):
        Y_post[i] = sess.run(model_pred, {model_X: xvalid_features})
y_predict = Y_post.mean(axis=0)
y_predict = np.reshape(y_predict, [-1, 1])
y_predict[y_predict >= 0.5] = 1
y_predict[y_predict < 0.5] = 0
accuracy_report(yvalid, y_predict)

"""# Build MC Dropout with ELMO layer

### Define ELMO layer
"""

# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)
# Import our dependencies
import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
import os
import re
from keras import backend as K
import keras.layers as layers
from keras.models import Model, load_model
from keras.engine import Layer
import numpy as np

np.random.seed(1)
# Initialize session
sess = tf.Session()
K.set_session(sess)

class ElmoEmbeddingLayer(Layer):
    def __init__(self, **kwargs):
        self.dimensions = 1024
        self.trainable=True
        super(ElmoEmbeddingLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', 
                               trainable=self.trainable,
                               name="{}_module".format(self.name))

        self.trainable_weights += K.tf.trainable_variables(scope="^{}_module/.*".format(self.name))
        super(ElmoEmbeddingLayer, self).build(input_shape)

    def call(self, x, mask=None):
        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),
                      as_dict=True,
                      signature='default',
                      )['default']
        return result

    def compute_mask(self, inputs, mask=None):
        return K.not_equal(inputs, '--PAD--')

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.dimensions)

import warnings
import numpy as np
from keras import backend as K
from keras.models import Sequential
from keras.layers.core import Dense
from keras.layers.core import Dropout
from keras.regularizers import l2
from keras.callbacks import EarlyStopping
import keras.layers as layers
from sklearn.model_selection import KFold
np.random.seed(42)
warnings.filterwarnings("ignore")

class elmo_net:
    def __init__(self, input_shape, output_shape, n_hidden, n_epochs=50):
        self.dropout = 0.5
        self.batch_size = 4
        self.n_epochs = n_epochs
        
        input_text = layers.Input(shape=(1,), dtype="string")
        embedding = ElmoEmbeddingLayer()(input_text)
        dense = layers.Dense(n_hidden, activation='relu')(embedding)
        dropout = layers.Dropout(self.dropout)(dense)
        dense2 = layers.Dense(n_hidden, activation='relu')(dropout)
        dropout2 = layers.Dropout(self.dropout)(dense2)
        pred = layers.Dense(1, activation='sigmoid')(dropout2)

        model = Model(inputs=[input_text], outputs=pred)

        model.compile(loss='binary_crossentropy', 
                      optimizer='adam', 
                      metrics=['accuracy'])
        
        self.model = model
        
    def fit(self, x_train, y_train):
        self.model.fit(x_train, y_train,
                       batch_size=self.batch_size,
                       nb_epoch=self.n_epochs,
                       verbose=0)
        
    def predict(self, X_test):
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([X_test, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        return MC_pred

"""## Cross-validation for MC Dropout with ELMo"""

corpus = df['normalized_text'].values.copy()
labels = df['HS'].values.copy()

X = corpus.copy()
y = labels
kf = KFold(n_splits=5, random_state=42, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []


for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_text = [' '.join(t.split()[0:]) for t in x_train.tolist()]
    train_text = np.array(train_text, dtype=object)[:, np.newaxis]
    test_text = [' '.join(t.split()[0:]) for t in x_test.tolist()]
    test_text = np.array(test_text, dtype=object)[:, np.newaxis]

    model = elmo_net(input_shape=train_text.shape[0], 
                     output_shape=1, n_hidden=1024, 
                     n_epochs=50)
    model.fit(train_text, y_train)
    
    predictions = model.predict(test_text)
    predictions = predictions.reshape(-1, 1)
    predictions[predictions >= 0.5] = 1
    predictions[predictions < 0.5] = 0

    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('FFNN Overall with ELMo')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""# SVM and Logistic with ELMo

## Define ELMo
"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate
import tensorflow_hub as hub
import tensorflow as tf
np.random.seed(1)

elmo = hub.Module("https://tfhub.dev/google/elmo/2")

def get_embedding_from_elmo(words_to_embed, elmo=elmo):
#     embedding_tensor = elmo(words_to_embed) # <-- removed other params
    embeddings = []
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for w in words_to_embed:
            try:
                embedding = sess.run(elmo(w))
                embeddings.append(embedding)
            except:
                print(w)
    sess.close() 
    return np.array(embeddings)

words_to_embed = [["dog", "cat"], ["sloth"]] 
embeddings = get_embedding_from_elmo(words_to_embed)
print(embeddings)
print(embeddings.shape)

"""## Cross-Validation for Logistic Regression"""

corpus = df['normalized_text'].values
labels = df['HS'].values

dataset = corpus.copy()
y = labels

embeddings = elmo(
    corpus.copy(),
    signature="default",
    as_dict=True)["default"]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    X = sess.run(embeddings)
sess.close()
print(X)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []
X_data = X.copy()

kf = KFold(n_splits=5, random_state=4, shuffle=True)
for train_index, test_index in kf.split(X_data, y):
    x_train, x_test = X_data[train_index], X_data[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    logistic = LogisticRegression()
    predictions = train_predict_evaluate_model(classifier=logistic, 
                                               train_features=x_train,
                                               train_labels=y_train,
                                               test_features=x_test,
                                               test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
print('Ligistic Regression Overall with ELMo')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""## Cross-Validation for SVM"""

kf = KFold(n_splits=5, random_state=4, shuffle=True)

count = 0
mean_acc = []
f1s = []
precis = []
recalls = []
X_data = X.copy()
for train_index, test_index in kf.split(X_data, y):
    x_train, x_test = X_data[train_index], X_data[test_index]
    y_train, y_test = y[train_index], y[test_index]

    svm = SVC(kernel='linear')
    predictions = train_predict_evaluate_model(classifier=svm, 
                                               train_features=x_train,
                                               train_labels=y_train,
                                               test_features=x_test,
                                               test_labels=y_test)
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    
    
print('SVM Overall with ELMo')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""# LSTM MC Dropout with ELMo

## Define ELMo Embedding model
"""

import keras
# Import our dependencies
import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
from keras import backend as K
import keras.layers as layers
from keras.models import Model, load_model
from keras.engine import Layer
import numpy as np
from keras.layers import LSTM, Dropout
from keras.layers import Embedding, Dense, TimeDistributed
from keras.layers import Lambda
from keras.layers.merge import add
from keras.regularizers import l2
np.random.seed(1)

# Initialize session
sess = tf.Session()
K.set_session(sess)

def ELMoEmbedding(x):
    elmo_model = hub.Module("https://tfhub.dev/google/elmo/2", 
                            trainable=True)
    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), 
                      signature='default',
                      as_dict=True)["elmo"]

class ElmoEmbeddingLayer(keras.engine.Layer):
    def __init__(self, **kwargs):
        self.dimensions = 1024
        self.trainable = True
        super(ElmoEmbeddingLayer, self).__init__(**kwargs)
        
    def build(self, input_shape):
        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', 
                               trainable=self.trainable, 
                               name="{}_module".format(self.name))
        self.trainable_weights += K.tf.trainable_variables(scope="^{}_module/.*".format(self.name))
        super(ElmoEmbeddingLayer, self).build(input_shape)
    
    def call(self, x, mask=None):
        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),
                      as_dict=True,
                      signature='default',
                      )['elmo']
        return result
    
    def compute_mask(self, inputs, mask=None):
        return K.not_equal(inputs, '--PAD--')
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.dimensions)

"""## Define LSTM with MC Dropout"""

def build_LSTM_MC():
    input_text = layers.Input(shape=(1,), dtype='string')
    embedding = layers.Lambda(ELMoEmbedding, output_shape=(None, 1024))(input_text) 
    x = LSTM(units=1024, 
             kernel_regularizer=l2(1e-4),
             bias_regularizer=l2(1e-4),
             return_sequences=False,
             recurrent_dropout=0.2, 
             dropout=0.5)(embedding)
    dense = Dense(1024, kernel_regularizer=l2(1e-4),
                 bias_regularizer=l2(1e-4))(x)
    dropout = Dropout(0.5)(dense)
    out = Dense(1, activation="sigmoid")(dropout)
    model = Model(input_text, out)

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics= ['acc'])
    return model

    
def lstm_fit(model, X_train_pad,y_train):
    model.fit(X_train_pad,
              y_train,
              shuffle=True,
              epochs=50,
              batch_size=4,
              verbose=0
             )
    return model

def lstm_predict(model, X_test_pad, y_test):
    T = 1000
    predict_stochastic = K.function([model.layers[0].input,
                                    K.learning_phase()],
                                    [model.layers[-1].output])
    Yt_hat = np.array([predict_stochastic([X_test_pad, 1]) for _ in range(T)])
    MC_pred = np.mean(Yt_hat, axis=0)

    MC_pred = MC_pred.reshape(-1, 1)

    MC_pred[MC_pred >= 0.5] = 1
    MC_pred[MC_pred < 0.5] = 0
    return accuracy_report(y_test, MC_pred)

"""## Cross validation for LSTM with MC Dropout"""

corpus = df['normalized_text'].values
labels = df['HS'].values

X = corpus.copy()
y = labels

kf = KFold(n_splits=5, random_state=42, shuffle=True)
count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_text = [' '.join(t.split()[0:]) for t in x_train.tolist()]
    train_text = np.array(train_text, dtype=object)[:, np.newaxis]
    test_text = [' '.join(t.split()[0:]) for t in x_test.tolist()]
    test_text = np.array(test_text, dtype=object)[:, np.newaxis]

    model = build_LSTM_MC()
    print(model.summary())
    lstm_fit(model, train_text, y_train)
    print('eps ', count + 1, ' fitting') 
    acc, f1, recall, precision = lstm_predict(model, test_text, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    count += 1
    
    
print('FFNN Overall with ELMo')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""# LSTM

## Define LSTM model with ELMo Embedding
"""

import keras
import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
from keras import backend as K
import keras.layers as layers
from keras.models import Model, load_model
from keras.engine import Layer
import numpy as np
from keras.layers import LSTM, Dropout
from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda
from keras.layers.merge import add
from keras.regularizers import l2

np.random.seed(1)
# Initialize session
sess = tf.Session()
K.set_session(sess)


class LSTM_Model:
    def __init__(self):
        self.model = self._build()
        
    def _build(self):
        input_text = layers.Input(shape=(1,), dtype='string')
        embedding = layers.Lambda(ELMoEmbedding, output_shape=(None, 1024))(input_text) 
        x = LSTM(units=1024, 
                 return_sequences=False,
                 recurrent_dropout=0.1)(embedding)
        dense = Dense(1024)(x)
        out = Dense(1, activation="sigmoid")(dropout)
        model = Model(input_text, out)
        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics= ['acc'])
        return model

    def fit(self, X_train_pad,y_train):
        self.model.fit(X_train_pad,
                       y_train,
                       shuffle=True,
                       epochs=50,
                       batch_size=4,
                       verbose=0
                     )

    def predict(self, X_test_pad, y_test):
        predictions = self.model.predict(X_test_pad)
        predictions = predictions.reshape(-1, 1)
        predictions[predictions >= 0.5] = 1
        predictions[predictions < 0.5] = 0
        return accuracy_report(y_test, predictions)

"""## Cross validation for LSTM"""

corpus = df['normalized_text'].values
labels = df['HS'].values

X = corpus.copy()
y = labels

kf = KFold(n_splits=5, random_state=42, shuffle=True)
count = 0
mean_acc = []
f1s = []
precis = []
recalls = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    train_text = [' '.join(t.split()[0:]) for t in x_train.tolist()]
    train_text = np.array(train_text, dtype=object)[:, np.newaxis]
    test_text = [' '.join(t.split()[0:]) for t in x_test.tolist()]
    test_text = np.array(test_text, dtype=object)[:, np.newaxis]

    model = LSTM_Model()
    model.fit(train_text, y_train)
    print('eps ', count + 1, ' fitting') 
    acc, f1, recall, precision = model.predict(test_text, y_test)
    mean_acc.append(acc)
    f1s.append(f1)
    precis.append(precision)
    recalls.append(recall)
    count += 1
    
    
print('FFNN Overall with ELMo')
print('Acc=', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('f1 score=', np.mean(f1s), ', std=', np.std(f1s))
print('Precision=', np.mean(precis), ', std=', np.std(precis))
print('Recalls=', np.mean(recalls), ', std=', np.std(recalls))

"""# Universal Sentences Encoder"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd

import sklearn
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import keras.layers as layers
from keras.models import Model
from keras import backend as K

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

np.random.seed(1)
tf.random.set_random_seed(1)
tf.reset_default_graph()

dataset = pd.read_csv('/content/gdrive/My Drive/cleaned_text_v3.csv')
del dataset['Unnamed: 0']
dataset.head()

"""## Define Universal encoder Module"""

# module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" 
# module_url = "https://tfhub.dev/google/nnlm-en-dim128/1"
module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3"

def get_embedding(module_url, texts):
    embed = hub.Module(module_url)
    X = texts
    tf.logging.set_verbosity(tf.logging.ERROR)

    with tf.Session() as session:
        session.run([tf.global_variables_initializer(), 
                    tf.tables_initializer()])
        message_embeddings = session.run(embed(X))
    session.close()
    text_output = np.array(message_embeddings)
    
    return text_output

"""## LSTM"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D
from keras.layers import LSTM

LSTM_DIM = 1024 # total LSTM units

class LSTM_No_MC():
    def __init__(self):
        self._build_LSTM_model()

    def _build_LSTM_model(self):
        self.model = Sequential() 
        self.model.add(LSTM(LSTM_DIM, dropout=0.2, 
                       recurrent_dropout=0.2)) 
        self.model.add(Dense(1024, activation='relu')) 
        self.model.add(Dropout(0.2)) 
        self.model.add(Dense(1, activation='sigmoid'))
        self.model.compile(loss='binary_crossentropy', 
                            optimizer='adam', 
                            metrics=['accuracy'])
        return self
    
    def fit(self, train_X, train_y):
        self.model.fit(train_X, train_y, 
              epochs=50, 
              batch_size=4, 
              shuffle=True, 
              verbose=0)

    def predict(self, x_test):
        predictions = self.model.predict(x_test)
        return predictions
    
    def evaluate(self, x_test, y_test):
        predictions = self.predict(x_test)
        predictions[predictions >= 0.5] = 1
        predictions[predictions < 0.5] = 0
    
        acc, f1, recall, precision = accuracy_report(y_test, predictions)
        return acc, f1, recall, precision

"""### LSTM cross validation"""

def lstm_cv(X, y):
    kf = KFold(n_splits=5, random_state=42, 
               shuffle=True)
    count = 0
    mean_acc = []
    f1s = []
    recalls = []
    precis = []
    for train_index, test_index in kf.split(X, y):
        x_train, x_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        input_length = x_train.shape[1]
        x_train = np.reshape(x_train,
                                (x_train.shape[0], 1, 
                                 x_train.shape[1]))

        x_test= np.reshape(x_test, (x_test.shape[0], 1, 
                                    x_test.shape[1]))
        print('*'*50)
        model = LSTM_No_MC()
        model.fit(x_train, y_train)

        # Prediction on the test set.
        print('*'*10)
        print('Evaluation model ', count + 1, '/5')
        count += 1
        # predictions = model.predict(x_test)

        acc, f1, recal, precision = model.evaluate(x_test, y_test)

        mean_acc.append(acc)
        f1s.append(f1)
        recalls.append(recal)
        precis.append(precision)

    print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
    print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
    print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
    print('Precisions=', np.mean(precis), ', std=', np.std(precis))

texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

lstm_cv(X, y)

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
tf.reset_default_graph()

from tensorflow.python.framework import ops
ops.reset_default_graph()
sess = tf.InteractiveSession()

kf = KFold(n_splits=5, random_state=22, 
           shuffle=True)
texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    print('*'*50)
    model = LogisticRegression()
    model.fit(x_train, y_train)
    
    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    # predictions = model.predict(x_test)
    
    print('Evaluation results:')
    
    prediction = model.predict_proba(x_test) # predicting on the validation set
    prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.3 than 1 else 0
    prediction_int = prediction_int.astype(np.int)
     
    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)

print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precisions=', np.mean(precis), ', std=', np.std(precis))
sess.close()

"""## SVM"""

from tensorflow.python.framework import ops
tf.reset_default_graph()
ops.reset_default_graph()
sess = tf.InteractiveSession()

from sklearn.svm import SVC

kf = KFold(n_splits=5, random_state=24, 
           shuffle=True)
texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    print('*'*50)
    svm = SVC()
    model.fit(x_train, y_train)
    
    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    
    print('Evaluation results:')
    
    prediction = model.predict_proba(x_test) # predicting on the validation set
    prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.3 than 1 else 0
    prediction_int = prediction_int.astype(np.int)
    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)
    
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)

print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precisions=', np.mean(precis), ', std=', np.std(precis))
sess.close()

"""## LSTM MC Dropout"""

from keras.optimizers import SGD, RMSprop, Adagrad
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, SpatialDropout1D
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU, SimpleRNN
from keras.regularizers import l2
from keras import backend as K
np.random.seed(1)

class LSTM_MC:
    def __init__(self, input_length):
        self.model = self.build()
        
    def build(self):
        print('Build model...')
        p_W, p_U, p_dense, p_emb = 0.75, 0.75, 0.5, 0.5
        weight_decay, batch_size, maxlen = 1e-4, 10, 500
        model = Sequential()
        model.add(LSTM(1024, input_shape=(1, input_length),
                       kernel_regularizer=l2(weight_decay), 
                       recurrent_regularizer=l2(weight_decay),
                       dropout=p_W, 
                       recurrent_dropout=p_U))
        model.add(Dropout(p_dense))
        model.add(Dense(1024, 
                        kernel_regularizer=l2(weight_decay), 
                        activation='relu'
                       ))
        model.add(Dropout(p_dense))
        model.add(Dense(1, kernel_regularizer=l2(weight_decay), 
                        activation='sigmoid'
                       ))
        model.compile(loss='binary_crossentropy', 
                      optimizer='adam',
                      metrics=['acc'])
        return model
    
    def fit(self, train_X, train_y):
        input_length = train_X.shape[1]
        train_X = np.reshape(train_X,
                                (train_X.shape[0], 1, train_X.shape[1]))
        self.model.fit(train_X, train_y, 
              batch_size=4, 
              shuffle=True,
              epochs=50,
              verbose=0)

    def predict(self, test_X):
        input_length = test_X.shape[1]
        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        MC_pred = MC_pred.reshape(-1, 1)
        MC_pred[MC_pred >= 0.5] = 1
        MC_pred[MC_pred < 0.5] = 0
        return MC_pred

texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

kf = KFold(n_splits=5, random_state=42, 
           shuffle=True)

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print('*'*50)
    input_length = x_train.shape[1]
    model = LSTM_MC(input_length)
    model.fit(x_train, y_train)
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    predictions = model.predict(x_test)
    print('Evaluation results:')
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)
    
print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precision=', np.mean(precis),', std=', np.std(precis))

"""# Universal Sentences Encoder 2"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd

import sklearn
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import keras.layers as layers
from keras.models import Model
from keras import backend as K

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

np.random.seed(1)
tf.random.set_random_seed(1)
tf.reset_default_graph()

dataset = pd.read_csv('/content/gdrive/My Drive/cleaned_text_v3.csv')
del dataset['Unnamed: 0']
dataset.head()

"""## Define Universal encoder Module"""

# module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" 
module_url = "https://tfhub.dev/google/nnlm-en-dim128/1"
# module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3"

def get_embedding(module_url, texts):
    embed = hub.Module(module_url)
    X = texts
    tf.logging.set_verbosity(tf.logging.ERROR)

    with tf.Session() as session:
        session.run([tf.global_variables_initializer(), 
                    tf.tables_initializer()])
        message_embeddings = session.run(embed(X))
    session.close()
    text_output = np.array(message_embeddings)
    
    return text_output

"""## LSTM MC Dropout"""

from keras.optimizers import SGD, RMSprop, Adagrad
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, SpatialDropout1D
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU, SimpleRNN
from keras.regularizers import l2
from keras import backend as K
np.random.seed(1)

class LSTM_MC:
    def __init__(self, input_length):
        self.model = self.build()
        
    def build(self):
        print('Build model...')
        p_W, p_U, p_dense, p_emb = 0.75, 0.75, 0.5, 0.5
        weight_decay, batch_size, maxlen = 1e-4, 10, 500
        model = Sequential()
        model.add(LSTM(256, input_shape=(1, input_length),
                       kernel_regularizer=l2(weight_decay), 
                       recurrent_regularizer=l2(weight_decay),
                       dropout=p_W, 
                       recurrent_dropout=p_U))
        model.add(Dropout(p_dense))
        model.add(Dense(256, 
                        kernel_regularizer=l2(weight_decay), 
                        activation='relu'
                       ))
        model.add(Dropout(p_dense))
        model.add(Dense(1, 
                        kernel_regularizer=l2(weight_decay), 
                        activation='sigmoid'
                       ))
        model.compile(loss='binary_crossentropy', 
                      optimizer='adam',
                      metrics=['acc'])
        return model
    
    def fit(self, train_X, train_y):
        input_length = train_X.shape[1]
        train_X = np.reshape(train_X,
                                (train_X.shape[0], 1, train_X.shape[1]))
        self.model.fit(train_X, train_y, 
              batch_size=10, 
              shuffle=True,
              epochs=100,
              verbose=0)

    def predict(self, test_X):
        input_length = test_X.shape[1]
        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        MC_pred = MC_pred.reshape(-1, 1)
        MC_pred[MC_pred >= 0.5] = 1
        MC_pred[MC_pred < 0.5] = 0
        return MC_pred

texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

np.random.seed(1)
kf = KFold(n_splits=5, random_state=42, 
           shuffle=True)

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print('*'*50)
    input_length = x_train.shape[1]
    model = LSTM_MC(input_length)
    model.fit(x_train, y_train)
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    predictions = model.predict(x_test)
    print('Evaluation results:')
    acc, f1, recall, precision = accuracy_report(y_test, predictions)
    
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)
    
print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precision=', np.mean(precis),', std=', np.std(precis))

"""## LSTM"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D
from keras.layers import LSTM

LSTM_DIM = 256 # total LSTM units

class LSTM_No_MC():
    def __init__(self):
        self._build_LSTM_model()

    def _build_LSTM_model(self):
        self.model = Sequential() 
        self.model.add(LSTM(LSTM_DIM, dropout=0.2, 
                       recurrent_dropout=0.2)) 
        self.model.add(Dense(256, activation='relu')) 
        self.model.add(Dropout(0.2)) 
        self.model.add(Dense(1, activation='sigmoid'))
        self.model.compile(loss='binary_crossentropy', 
                            optimizer='adam', 
                            metrics=['accuracy'])
        return self
    
    def fit(self, train_X, train_y):
        self.model.fit(train_X, train_y, 
              epochs=50, 
              batch_size=4, 
              shuffle=True, 
              verbose=0)

    def predict(self, x_test):
        predictions = self.model.predict(x_test)
        return predictions
    
    def evaluate(self, x_test, y_test):
        predictions = self.predict(x_test)
        predictions[predictions >= 0.5] = 1
        predictions[predictions < 0.5] = 0
    
        acc, f1, recall, precision = accuracy_report(y_test, predictions)
        return acc, f1, recall, precision

"""### LSTM cross validation"""

def lstm_cv(X, y):
    kf = KFold(n_splits=5, random_state=42, 
               shuffle=True)
    count = 0
    mean_acc = []
    f1s = []
    recalls = []
    precis = []
    for train_index, test_index in kf.split(X, y):
        x_train, x_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        input_length = x_train.shape[1]
        x_train = np.reshape(x_train,
                                (x_train.shape[0], 1, 
                                 x_train.shape[1]))

        x_test= np.reshape(x_test, (x_test.shape[0], 1, 
                                    x_test.shape[1]))
        print('*'*50)
        model = LSTM_No_MC()
        model.fit(x_train, y_train)

        # Prediction on the test set.
        print('*'*10)
        print('Evaluation model ', count + 1, '/5')
        count += 1
        # predictions = model.predict(x_test)

        acc, f1, recal, precision = model.evaluate(x_test, y_test)

        mean_acc.append(acc)
        f1s.append(f1)
        recalls.append(recal)
        precis.append(precision)

    print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
    print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
    print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
    print('Precisions=', np.mean(precis), ', std=', np.std(precis))

texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

np.random.seed(1)
lstm_cv(X, y)

"""## Logistic Regression"""

np.random.seed(1)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
tf.reset_default_graph()

from tensorflow.python.framework import ops
ops.reset_default_graph()
sess = tf.InteractiveSession()

kf = KFold(n_splits=5, random_state=22, 
           shuffle=True)
texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    print('*'*50)
    model = LogisticRegression()
    model.fit(x_train, y_train)
    
    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    # predictions = model.predict(x_test)
    
    print('Evaluation results:')
    
    prediction = model.predict_proba(x_test) # predicting on the validation set
    prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.3 than 1 else 0
    prediction_int = prediction_int.astype(np.int)
     
    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)

print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precisions=', np.mean(precis), ', std=', np.std(precis))
sess.close()

"""## SVM"""

np.random.seed(1)
from tensorflow.python.framework import ops
tf.reset_default_graph()
ops.reset_default_graph()
sess = tf.InteractiveSession()

from sklearn.svm import SVC

kf = KFold(n_splits=5, random_state=24, 
           shuffle=True)
texts = dataset['normalized_text'].values
X = get_embedding(module_url, texts)
y = dataset['HS'].values

count = 0
mean_acc = []
f1s = []
recalls = []
precis = []

for train_index, test_index in kf.split(X, y):
    x_train, x_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    print('*'*50)
    svm = SVC(kernel='linear')
    model.fit(x_train, y_train)
    
    # Prediction on the test set.
    print('*'*10)
    print('Evaluation model ', count + 1, '/5')
    count += 1
    
    print('Evaluation results:')
    
    prediction = model.predict_proba(x_test) # predicting on the validation set
    prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.3 than 1 else 0
    prediction_int = prediction_int.astype(np.int)
    acc, f1, recall, precision = accuracy_report(y_test, prediction_int)
    
    mean_acc.append(acc)
    f1s.append(f1)
    recalls.append(recall)
    precis.append(precision)

print('Acc = ', np.mean(mean_acc), ', std = ', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precisions=', np.mean(precis), ', std=', np.std(precis))
sess.close()

"""# Convolution Network"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from collections import defaultdict
import re
import sys
import os
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Input,Embedding,Dense,Flatten
from sklearn.metrics import accuracy_score,classification_report
import keras.layers as layers

import matplotlib.pyplot as plt
plt.switch_backend('agg')
np.random.seed(1)
# os.environ['KERAS_BACKEND']='theano' # Why theano why not
# %matplotlib inline

embed_dim = 100
maxlen = 2000
vocab_size = 1000
trainable_param = False
workers = 4
window = 1

X = df['normalized_text'].values
y = df['HS'].values

sentences_as_words=[]
word_to_index={}
count=1
for sent in X:
    temp = sent.split()
    sentences_as_words.append(temp)
for sent in sentences_as_words:
    for word in sent:
        if word_to_index.get(word,None) is None:
            word_to_index[word] = count
            count +=1
index_to_word = {v:k for k,v in word_to_index.items()}
sentences=[]
for i in range(len(sentences_as_words)):
    temp = [word_to_index[w] for w in sentences_as_words[i]]
    sentences.append(temp)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(sentences, y)

X_train_pad = pad_sequences(X_train,maxlen=maxlen,padding="post")
X_test_pad = pad_sequences(X_test,maxlen=maxlen,padding="post")

print(sentences_as_words)

from gensim.models import Word2Vec
model = Word2Vec(sentences=sentences_as_words, 
                 size = embed_dim,
                 workers = workers,
                 window = window)

embedding_matrix = np.zeros((vocab_size, embed_dim))
for word, i in word_to_index.items():
    try:
        embedding_vector = w2vmodel[word]
    except:
        pass
    try:
        if embedding_vector is not None:
            embedding_matrix[i]=embedding_vector
    except:
        pass

embedding_layer = Embedding(vocab_size,
                            embed_dim,
                            weights=[embedding_matrix],
                            trainable=True)

# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), 
#                        dtype='int32')
sequence_input = Input(shape=(X_train_pad.shape[1],))
embedded_sequences = embedding_layer(sequence_input)
l_cov1= Conv1D(128, 3, activation='relu')(embedded_sequences)
dropout1 = Dropout(0.5)(l_cov1)
l_pool1 = MaxPooling1D(3)(dropout1)
l_cov2 = Conv1D(128, 3, activation='relu')(l_pool1)
dropout2 = Dropout(0.5)(l_cov2)
l_pool2 = MaxPooling1D(3)(dropout2)
l_cov3 = Conv1D(128, 3, activation='relu')(l_pool2)
dropout3 = Dropout(0.5)(l_cov3)
l_pool3 = MaxPooling1D(3)(dropout3)  # global max pooling
l_flat = Flatten()(l_pool3)
l_dense = Dense(128, activation='relu')(l_flat)
preds = Dense(1, activation='sigmoid')(l_dense)

model = Model(sequence_input, preds)
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

print("Simplified convolutional neural network")
model.summary()

model.fit(X_train_pad,y_train,
          epochs=50,
          batch_size=2,
          validation_split=0.1,
          verbose=0)

predictions = model.predict(X_test_pad)
predictions = [0 if i<0.5 else 1 for i in predictions]
print("Accuracy: ",accuracy_score(y_test,predictions))
print("Classification Report: ",classification_report(y_test,predictions))

import tensorflow as tf
import tensorflow_hub as hub
def ELMoEmbedding(x):
    elmo_model = hub.Module("https://tfhub.dev/google/elmo/1", trainable=True)
    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature="default", as_dict=True)["default"]

input_text = layers.Input(shape=(1,), dtype=tf.string)
embed_seq = layers.Lambda(ELMoEmbedding, output_shape=(1024,))(input_text)
x = Dense(256,activation ="relu")(embed_seq)
preds = Dense(1,activation="sigmoid")(x)
model = Model(input_text,preds)

model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])

# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), 
#                        dtype='int32')
sequence_input = Input(shape=(1, ), dtype=tf.string)
embeded_sequences = layers.Lambda(ELMoEmbedding, 
                                  output_shape=(1024,))(sequence_input)

# embedded_sequences = embedding_layer(sequence_input)
l_cov1= Conv1D(128, 3, activation='relu')(embedded_sequences)
l_pool1 = MaxPooling1D(3)(l_cov1)
l_cov2 = Conv1D(128, 3, activation='relu')(l_pool1)
l_pool2 = MaxPooling1D(3)(l_cov2)
l_cov3 = Conv1D(128, 3, activation='relu')(l_pool2)
l_pool3 = MaxPooling1D(3)(l_cov3)  # global max pooling
l_flat = Flatten()(l_pool3)
l_dense = Dense(128, activation='relu')(l_flat)
preds = Dense(1, activation='sigmoid')(l_dense)

model = Model(sequence_input, preds)
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

print("Simplified convolutional neural network")
model.summary()