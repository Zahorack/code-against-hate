# -*- coding: utf-8 -*-
"""EN3 LSTM MCD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tv8ca3RfrCOzHXQI-jklnCgSnNoASNeE

# Import Library
"""

import torch
import numpy as np
import copy
from torch.nn import functional as F
from torch.nn.modules.module import Module
from torch.nn.modules.activation import MultiheadAttention
from torch.nn.modules.container import ModuleList
from torch.nn.init import xavier_uniform_
from torch.nn.modules.dropout import Dropout
from torch.nn.modules.linear import Linear
from torch.nn.modules.normalization import LayerNorm
from torch.utils.data import DataLoader, Dataset
import logging
logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')
logging.getLogger().setLevel(logging.INFO)

import math
from torch.autograd import Variable
import re
import pandas as pd
torch.manual_seed(2)

"""# Access to GDrive"""

# Access to resources
# from google.colab import drive
# drive.mount('/content/gdrive', force_remount=True)

# Load stop words and more (will be added later)
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print(stop_words)

# Cleaning data
torch.manual_seed(2)
import unicodedata
import re
import gensim
from nltk import tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

del stop_words[stop_words.index('not')]
del stop_words[stop_words.index('your')]

# Remove unwanted noise
stop_words.append('rt')
stop_words.append('wow')
stop_words.append('ok')
stop_words.append('mo')
stop_words.append('dm')
stop_words.append('idgaf')

CONTRACTION_MAP = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he had",
"he'd've": "he would have",
"he'll": "hehe will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "hey would",
"they'd've": "they would have",
"they'll": "tthey will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what'll've": "what will have",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"when's": "when is",
"when've": "when have",
"where'd": "where did",
"where's": "where is",
"where've": "where have",
"who'll": "who will",
"who'll've": "who will have",
"who's": "who is",
"who've": "who have",
"why's": "why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you would",
"you'd've": "you would have",
"you'll": "you will",
"you'll've": "you will have",
"you're": "you are",
"you've": "you have",
"fvcking": "fucking",
"fking": "fucking",
# Correction
    # "ill": "I will",
    "seriois": "serious",
    #"mo": "my",
    #"ass": "asss",
    "lmaoooooooo": "lmao",
    "uncomf": "uncomfortable",
    "pls": "please",
    "lowlife": "low life",
    "puss": "pussy",
    # Hashtash expand
    "ihatefemales": "I hate females",
    "yesallwomen": "yes all women",
    "sendthemhome": "send them home",
    "stoptheinvasion": "stop the invation",
    "buildthewall": "build the wall",
    "womensuck": "women suck",
    "stopimmigration": "stop immigration",
    "sendthemback": "send them back"
}

# Expand the contractions
def expand_contractions(s, contractions_dict=CONTRACTION_MAP):
    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, s)

# Sentences to words
def sent_to_words(sentences):
    for sentence in sentences:
        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))
        
# tokenize
def sent_tokenize(sents):
    return tokenize.sent_tokenize(sents)
  
# Function to remove some unwanted info
def remove_info(data, remove_list):
    for i, sent in enumerate(data):
        for com in remove_list:
            if com.strip() != '':
                sent = sent.lower().replace(com.lower(), '')
        data[i] = sent
    return data

# Remove special characters; only get ASCII
def remove_accents(input_str):
    nfkd_form = unicodedata.normalize('NFKD', str(input_str))
    only_ascii = nfkd_form.encode('ASCII', 'ignore')
    only_ascii = str(only_ascii)[2:-1]
    return only_ascii

# Remove emails, link, tweeter account, ..
def clean(texts):
    # Remove Emails
    data = [re.sub('\S*@\S*\s?', '', sent.lower().strip()) for sent in texts]
    # Remove @username
    data = [re.sub('@\S*\s?', '', sent) for sent in data]
    # Remove link
    data = [re.sub(r'http\S+', '', sent) for sent in data]
    # Remove new line characters
    data = [re.sub('\s+', ' ', sent) for sent in data]
    # Remove multi dots
    data = [re.sub('\.\.', '', sent) for sent in data]
    # Remove distracting quotes
    # data = [re.sub("\"", "", sent) for sent in data]
    # Remove #
    data = [re.sub("#", "", sent) for sent in data]
    # Remove number
    data = [re.sub(r"\d+", '', sent) for sent in data]
    return data

# Func to remove stop words
def remove_stopwords(texts, stop_words=stop_words):
    return [' '.join([word.replace(' ', '') 
                for word in gensim.utils.simple_preprocess(str(doc)) \
                if word.replace(' ', '') not in stop_words]) for doc in texts]

# Func to lemmatize words using spacy
def lemmatization(texts, stop_words=None, 
                  allowed_postags=('NOUN', 'VERB', 'ADV')):
    nlp = en_core_web_md.load()
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc 
                            if token.pos_ in allowed_postags \
                              and token.lemma_ not in stop_words])
    return texts_out

# Func to lemmatize words using nltk
# Refer to https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
def lemmatization_nltk(sents):
    # input: sents -> list of sentences
    # output: list of lemmatized words
    def get_wordnet_pos(word):
        """Map POS tag to first character lemmatize() accepts"""
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV}

        return tag_dict.get(tag, wordnet.NOUN)

    lemmatized_words = []
    
    lemmatizer = WordNetLemmatizer()
    for sentence in sents:
        lemmatized_words.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) 
                                  for w in nltk.word_tokenize(sentence)]))
    return lemmatized_words

# Normalized pipeline
def normalization_pipeline(texts):
    normalized_texts = []
    # lower text
    normalized_texts = [text.lower() for text in texts]
    
    # Clean text: special words, numbers, ..
    normalized_texts = clean(normalized_texts)
    # Expand contraction 
    normalized_texts = [expand_contractions(text) for text in normalized_texts]
    normailzed_texts = [remove_accents(text) for text in normalized_texts]
    # Break paragraph into sentents
    normalized_texts = [sent_tokenize(text) for text in normalized_texts]
    
    # Remove stopwords
    normalized_texts = remove_stopwords(normalized_texts)
    
    normalized_texts = lemmatization_nltk(normalized_texts)
    return normalized_texts

"""# Defined Accuracy Report"""

import tensorflow as tf
import tensorflow_hub as hub

tf.compat.v1.disable_eager_execution()


import numpy as np
import pandas as pd

import sklearn
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split

# import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
# import seaborn as sns
# import keras.layers as layers
# from keras.models import Model
# from keras import backend as K

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

# np.random.seed(1)
# tf.random.set_random_seed(1)
# tf.reset_default_graph()

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

def accuracy_report(yvalid, prediction):
    cm1 = confusion_matrix(yvalid, prediction)
    print('Confusion Matrix :', cm1)

    total1=sum(sum(cm1))

    accuracy1=(cm1[0,0]+cm1[1,1])/total1
    print ('Accuracy : ', accuracy1)

    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    print('Sensitivity : ', sensitivity1 )

    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    print('Specificity : ', specificity1)
    print('                 ')
    print('Accuracy:', accuracy_score(yvalid, prediction))
    print('Precision:', precision_score(yvalid, prediction))
    print('Recall:', recall_score(yvalid, prediction))
    print('F1:', f1_score(yvalid, prediction))
    return accuracy_score(yvalid, prediction), f1_score(yvalid, prediction), \
            recall_score(yvalid, prediction), precision_score(yvalid, prediction)

"""# Read dataset"""

# Read data from file
import pandas as pd
from sklearn.utils import shuffle

# Point to the file in Google Drive

# filename='/content/gdrive/My Drive/EN_HS/big_dataset.csv'

# filename = '/home/zahorack/Projects/code-against-hate/big_dataset.csv'

filename = "C:\\Users\\oliver.holly.HQ\\Projects\\Python\\code-against-hate\\dataset\\big_dataset.csv"

df = pd.read_csv(filename, sep=',')

df.columns

print(len(df))
print(len(df[df['label']==1]))

torch.manual_seed(2)
corpus = df['tweet'].values
normalized_texts = normalization_pipeline(corpus)

df['normalized_text'] = normalized_texts

# Shullling 
# torch.manual_seed(8)
# df = shuffle(df)
# print(df.head())

# module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" 
module_url = "https://tfhub.dev/google/nnlm-en-dim128/1"
# module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/3"

def get_embedding(module_url, texts):
    embed = hub.Module(module_url)
    X = texts
    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

    with tf.compat.v1.Session() as session:
        session.run([tf.compat.v1.global_variables_initializer(),
                    tf.compat.v1.tables_initializer()])
        message_embeddings = session.run(embed(X))
    session.close()
    text_output = np.array(message_embeddings)
    
    return text_output

from sklearn.utils import shuffle
df= shuffle(df, random_state=100)
df.head()

df=df.reset_index(drop=True)
df.head()

texts = df['normalized_text'].values
X = get_embedding(module_url, texts)
#y = df['label'].values

X= pd.DataFrame(X)
X.head()

df.shape

df = pd.concat([df,X], axis=1)
df.head()

train = pd.DataFrame(df.iloc[:4900,:])
test = pd.DataFrame(df.iloc[4900:,:])

"""# Define LSTM MC"""

from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN
from tensorflow.keras.regularizers import l2
from tensorflow.keras import backend as K
np.random.seed(1)

class LSTM_MC:
    def __init__(self):
        self.model = self.build()
        
    def build(self):
        print('Build model...')
        p_W, p_U, p_dense, p_emb = 0.75, 0.75, 0.5, 0.5
        weight_decay, batch_size, maxlen = 1e-4, 10, 500
        model = Sequential()
        model.add(LSTM(1024, input_shape=(1, None),
                       kernel_regularizer=l2(weight_decay),
                       recurrent_regularizer=l2(weight_decay),
                       dropout=p_W,
                       recurrent_dropout=p_U))
        model.add(Dropout(p_dense))
        model.add(Dense(1024, 
                        kernel_regularizer=l2(weight_decay), 
                        activation='relu'
                       ))
        model.add(Dropout(p_dense))
        model.add(Dense(1, kernel_regularizer=l2(weight_decay), 
                        activation='sigmoid'
                       ))
        model.compile(loss='binary_crossentropy', 
                      optimizer='adam',
                      metrics=['accuracy'])
        return model
    
    def fit(self, train_X, train_y):
        input_length = train_X.shape[1]
        train_X = np.reshape(train_X,
                                (train_X.shape[0], 1, train_X.shape[1]))
        self.model.fit(train_X, train_y, 
              batch_size=2048, 
              shuffle=True,
              epochs=500,
              verbose=0)

    def predict(self, test_X):
        input_length = test_X.shape[1]
        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        MC_pred = MC_pred.reshape(-1, 1)
        MC_pred[MC_pred >= 0.5] = 1
        MC_pred[MC_pred < 0.5] = 0
        Yt_hat = Yt_hat.reshape(-1, 1)
        return Yt_hat

    def save(self, path):
        self.model.save(path)

x_train= train.iloc[:,4:132].values
x_test= test.iloc[:,4:132].values

y_train= train.iloc[:,1:2].values
y_test= test.iloc[:,1:2].values


new_model = tf.keras.models.load_model('model')
loss, acc = new_model.predict(x_test,  y_test)
print("Untrained model, accuracy: {:5.2f}%".format(100*acc))
print(loss)
print(acc)



print(y_train)


print(x_train.shape)
print(x_train)
print(y_train)

test['tweet']

input_length = x_train.shape[1]
model = LSTM_MC(input_length)
model.fit(x_train, y_train)

predictions = model.predict(x_test)

print(predictions)
model.save('model2')


loss, acc = model.evaluate(x_test,  y_test, verbose=2)
print("Untrained model, accuracy: {:5.2f}%".format(100*acc))
print(loss)
print(acc)


predictions.shape

MC_pred = predictions.reshape(1000,100)
pred=pd.DataFrame(MC_pred)
means=pred.mean(axis=0)
#pred.head()

pred= pred.T
pred.shape

x_test = pd.DataFrame(x_test)
y_test = pd.DataFrame(y_test)

x_test=x_test.reset_index(drop=True)
y_test=y_test.reset_index(drop=True)

text=test['tweet']
text= pd.DataFrame(text)
text=text.reset_index(drop=True)
text.shape

y_test

result = pd.concat([text,y_test, x_test, pred], axis=1, sort=False)

# result.to_csv(r'LSTM_MCD100.csv')

means[means >= 0.5] = 1
means[means < 0.5] = 0

acc, f1, recall, precision = accuracy_report(y_test, means)

from sklearn import metrics
import numpy as np
print("Accuracy:",metrics.accuracy_score(y_test, means))
print("Precision:",metrics.precision_score(y_test, means))
print("Recall:",metrics.recall_score(y_test, means))
print("F1 Score:", metrics.f1_score(y_test, means))

# kf = KFold(n_splits=5, random_state=42, 
#            shuffle=True)

# count = 0
# mean_acc = []
# f1s = []
# recalls = []
# precis = []

# for train_index, test_index in kf.split(X):
#     x_train, x_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     print('*'*50)
#     input_length = x_train.shape[1]
#     model = LSTM_MC(input_length)
#     model.fit(x_train, y_train)
#     print('*'*10)
#     print('Evaluation model ', count + 1, '/5')
#     count += 1
#     predictions = model.predict(x_test)
#     print('Evaluation results:')
#     acc, f1, recall, precision = accuracy_report(y_test, predictions)
    
#     mean_acc.append(acc)
#     f1s.append(f1)
#     recalls.append(recall)
#     precis.append(precision)
    
# print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))
# print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
# print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
# print('Precision=', np.mean(precis),', std=', np.std(precis))

"""# FastText"""

# Install packages
# !pip install xlrd
# !pip install gensim
# !pip install -U nltk
# !pip install spacy


########################################################################
    ##### Fast Embeddings!
    
# !pip install fasttext
import fasttext

# Loading the FastText pretrained models
model = fasttext.load_model('C:/Users/HP/Desktop/Paper/cc.hr.300.bin')
print(model['srbi'])

# Split test and train datasets
train = pd.DataFrame(df.iloc[:4000,:])
test = pd.DataFrame(df.iloc[4000:,:])

#train.shape
test.shape
#test.columns

x_train= train['normalized_text'].values
x_test= test['normalized_text'].values

y_train= train['label'].values
y_test= test['label'].values

print(y_train,x_train)

####### Reading data

# Read data from file
import pandas as pd
import numpy as np
import nltk


filename='C:/Users/HP/Desktop/Paper/Data/Selected/STY_vecernji_cleaned.csv'
df = pd.read_csv(filename)

df.head()
df.columns
df.shape

# Install packages
# !pip install xlrd
# !pip install gensim
# !pip install -U nltk
# !pip install spacy


########################################################################
    ##### Fast Embeddings!
    
# !pip install fasttext
import fasttext

# Loading the FastText pretrained models
model = fasttext.load_model('C:/Users/HP/Desktop/Paper/cc.hr.300.bin')
print(model['srbi']) 


# define function to average word vectors for a text document    
def average_word_vectors(words, model, num_features):
    feature_vector = np.zeros((num_features,),dtype="float32")
    nwords = 0.
    for word in words:
      #  if word in vocabulary:
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model[word])
    if nwords:
        feature_vector = np.divide(feature_vector, nwords)
    return feature_vector

average_word_vectors(['rvati', 'su', 'iz', 'perzije', 'ali', 'su', 'iz', 'ljubavi', 'prema'],model,num_features=300)

# generalize above function for a corpus of documents  
def averaged_word_vectorizer(corpus, model, num_features):
    #vocabulary = set(model.wv.index2word)
    features = [average_word_vectors(tokenized_sentence, model, 
                                     num_features)
                  for tokenized_sentence in corpus]
    return np.array(features)

corpus = df['normalized_text'].values.copy()
tokenized_corpus = [nltk.word_tokenize(sent) for sent in corpus if isinstance(sent, str)]


### Create Embeddings for our text 
emb= averaged_word_vectorizer(tokenized_corpus, model, 300)
emb.shape


### Combining the ebeddins with the HS label 

type(df)
type(emb)
total= pd.concat([df, pd.DataFrame(emb)], axis=1)
type(total)
total.shape
total.columns
#total['infringed_on_rule']= total['infringed_on_rule'].replace(np.nan, 0)

total= total.drop(['Unnamed: 0', 'comment_id', 'content'], axis=1)

total.head()
total['normalized_text']
# Change name of the column
total.rename(columns={'infringed_on_rule':'HS'}, inplace=True)

# Saving Total dataframe

total.to_csv(r'C:/Users/HP/Desktop/Paper/Data/comp_vecernji.csv')

##################################################################

        ### LASER Embeddings
        
encoder = SentenceEncoder(
    str(MODEL_PATH / "bilstm.93langs.2018-12-26.pt"),
    max_sentences=None,
    max_tokens=10000,
    cpu=False)        


###################################################################
    ############### Modeling
import pandas as pd
import numpy as np
import nltk
    
# Total
filename='C:/Users/HP/Desktop/Paper/Data/comp_vecernji.csv'
total = pd.read_csv(filename)

#total= total.drop(['Unnamed: 0'], axis=1)
total['normalized_text']
total.head()
total.columns
total.shape
                    
# Removed short comments
total4=total[total['normalized_text'].str.split().str.len()>5]
total4['normalized_text']
total.shape[0]-total4.shape[0]

#total4.to_csv(r'C:/Users/HP/Desktop/Paper/Data/totla4.csv')

total=total4

# Shuffle Dataframe
total = total.sample(frac=1).reset_index(drop=True)

total['normalized_text']

total1=total.dropna()
total.shape
total.shape[0]-total1.shape[0]

import random
random.seed(1)

total2 = total1.sample(n=3000)
total2.shape


# Split test and train datasets

train = pd.DataFrame(total2.iloc[:2400,:])
test = pd.DataFrame(total2.iloc[2400:,:])

#train = pd.DataFrame(total1.iloc[:118069,:])
#test = pd.DataFrame(total1.iloc[118069:,:])

train.shape
test.shape
test.columns

x_train= train.iloc[:,2:].values
x_test= test.iloc[:,2:].values

y_train= train['HS'].values
y_test= test['HS'].values


test_text= pd.DataFrame(test_text)
print(test_text) 
test_text.to_csv(r'C:/Users/HP/Desktop/Paper/Data/test_text.csv')


### Prepare functions
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
np.random.seed(1)

def prepare_datasets(corpus, labels, test_data_proportion=0.3):
    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,
                                                      test_size=test_data_proportion, 
                                                      random_state=42,
                                                      shuffle=True)
    return train_X, test_X, train_Y, test_Y

def get_metrics(true_labels, predicted_labels):
    print('Accuracy:', 
        np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))
    print('Precision:', 
        np.round(metrics.precision_score(true_labels,
                                         predicted_labels,
                                         average='weighted'),2))
    print('Recall:', 
        np.round(metrics.recall_score(true_labels,
                                      predicted_labels,
                                      average='weighted'),2))
    print('F1 Score:', 
        np.round(metrics.f1_score(true_labels,
                                  predicted_labels,
                                  average='weighted'),2))
    return

def train_predict_evaluate_model(classifier, train_features, 
                                 train_labels, test_features, test_labels):
    classifier.fit(train_features, train_labels)
    predictions = classifier.predict(test_features)

    return predictions

### Accuracy Report
    
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

def accuracy_report(yvalid, prediction):
    cm1 = confusion_matrix(yvalid, prediction)
    print('Confusion Matrix :', cm1)

    total1=sum(sum(cm1))

    accuracy1=(cm1[0,0]+cm1[1,1])/total1

    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])

    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    
    print('                 ')
    print('Accuracy:', accuracy_score(yvalid, prediction))
    print('Precision:', precision_score(yvalid, prediction))
    print('Recall:', recall_score(yvalid, prediction))
    print('F1:', f1_score(yvalid, prediction))
    return accuracy_score(yvalid, prediction), f1_score(yvalid, prediction), \
            recall_score(yvalid, prediction), precision_score(yvalid, prediction)


# Balance
total2.shape
total2.columns            
hs= total2[total2['HS']==1]
hs.shape
nonhs= total2[total2['HS']==0]        
nonhs.shape   

        

##############################################################################
        ### SVM
from sklearn.svm import SVC
    
svm = SVC(random_state=42, C=1.0, 
             kernel='rbf',
             gamma='scale')

#sklearn.svm.SVC (C=1.0, kernel=’rbf’, degree=3, gamma=’auto’)

svm_predictions = train_predict_evaluate_model(classifier=svm, 
                                                 train_features=x_train, 
                                                 train_labels=y_train,
                                                 test_features=x_test, 
                                                 test_labels=y_test)

acc, f1, recall, precision = accuracy_report(y_test, svm_predictions)



##############################################################################
        ### MCD LSTM
        
# Defining Model

# from keras.optimizers import SGD, RMSprop, Adagrad
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN
from tensorflow.keras.regularizers import l2
from tensorflow.keras import backend as K

np.random.seed(1)

class LSTM_MC:
    def __init__(self, input_length):
        self.model = self.build()
        
    def build(self):
        print('Build model...')
        p_W, p_U, p_dense, p_emb = 0.75, 0.75, 0.5, 0.5
        weight_decay, batch_size, maxlen = 1e-4, 10, 500
        model = Sequential()
        model.add(LSTM(1024, input_shape=(1, input_length),
                       kernel_regularizer=l2(weight_decay), 
                       recurrent_regularizer=l2(weight_decay),
                       dropout=p_W, 
                       recurrent_dropout=p_U))
        model.add(Dropout(p_dense))
        model.add(Dense(1024, 
                        kernel_regularizer=l2(weight_decay), 
                        activation='relu'
                       ))
        model.add(Dropout(p_dense))
        model.add(Dense(1, kernel_regularizer=l2(weight_decay), 
                        activation='sigmoid'
                       ))
        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics=['acc'])
        return model
    
    def fit(self, train_X, train_y):
        input_length = train_X.shape[1]
        train_X = np.reshape(train_X,
                                (train_X.shape[0], 1, train_X.shape[1]))
        self.model.fit(train_X, train_y, 
              batch_size=4, 
              shuffle=True,
              epochs=50,
              verbose=0)

    def predict(self, test_X):
        input_length = test_X.shape[1]
        test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))
        T = 1000
        predict_stochastic = K.function([self.model.layers[0].input,
                                        K.learning_phase()],
                                        [self.model.layers[-1].output])
        Yt_hat = np.array([predict_stochastic([test_X, 1]) for _ in range(T)])
        MC_pred = np.mean(Yt_hat, axis=0)
        MC_pred = MC_pred.reshape(-1, 1)
        MC_pred[MC_pred >= 0.5] = 1
        MC_pred[MC_pred < 0.5] = 0
        #return print('something')
        return Yt_hat 
        #return MC_pred        
        



print('Begin ')
input_length = x_train.shape[1]
model = LSTM_MC(input_length)
model.fit(x_train, y_train)

predictions = model.predict(x_test)
type(predictions)
print(predictions.shape)

pr= predictions.reshape(-1,1)
pr.shape

pred= predictions.reshape(-1, 1)
pred.shape
new= np.reshape(pred, (1000,300))
new.shape

new1= pd.DataFrame(new) 
type(y_test)
y_test=pd.DataFrame(y_test)

new2= new1.append(y_test , ignore_index=True)

y_test.to_csv(r'C:/Users/HP/Desktop/Paper/Data/y_test.csv')
new1.to_csv(r'C:/Users/HP/Desktop/Paper/Data/predictions.csv')


print('Evaluation results:')
acc, f1, recall, precision = accuracy_report(y_test, predictions)
    
    #mean_acc.append(acc)
    #f1s.append(f1)
    #recalls.append(recall)
    #precis.append(precision)
    
print('Acc = ', np.mean(mean_acc), ', std=', np.std(mean_acc))
print('F1 = ', np.mean(f1s), ', std=', np.std(f1s))
print('Recalls = ', np.mean(recalls), ', std=', np.std(recalls))
print('Precision=', np.mean(precis),', std=', np.std(precis))